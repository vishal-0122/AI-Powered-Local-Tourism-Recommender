# -*- coding: utf-8 -*-
"""using_DBSCAN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_Sa8h4CnsCsY-RXiO1TVdowoAC_-MDdt
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("data.csv")
df

# Checking for missing values
missing_values = df.isnull().sum()

# Checking for duplicate records
duplicate_records = df.duplicated().sum()

# Summary statistics for numerical columns
summary_statistics = df.describe()

# Display results
missing_values, duplicate_records, summary_statistics

# Dropping irrelevant columns
df.drop(columns=["Serial Number", "Establishment Year", "Weekly Off"], inplace=True, errors="ignore")

# Display the updated dataset structure
df.head()

"""## Feature Selection Completed
We now have six datasets, each focusing on a different segmentation approach:

### 1️. Geographic-Based Clustering  
- **(Zone, Number of Google Reviews)** → Popularity by region  
- **(City, Google Review Rating)** → Identify cities with good/bad reviews  

### 2️. Significance-Based Clustering  
- **(Significance, Entrance Fee)** → Cost comparison across significance types  
- **(Significance, Visit Duration)** → Time required for different place types  

### 3. Google Review-Based Clustering  
- **(Review Rating, Entrance Fee)** → Do expensive places get better reviews?  
- **(Review Count, Visit Duration)** → Do longer visits get more reviews?  

"""

from sklearn.preprocessing import LabelEncoder

# Encoding categorical variables
label_encoders = {}
categorical_columns = ["Zone", "City", "Significance"]

for col in categorical_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le  # Store encoders for future reference

# Display encoded data
df.head()

import seaborn as sns

# Box plots to check for outliers
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[["Entrance Fee in INR", "Number of google review in lakhs", "time needed to visit in hrs"]])
plt.xticks(rotation=45)
plt.title("Box Plot of Numerical Features")
plt.show()

# Log transformation to reduce skewness
df["Entrance Fee in INR"] = np.log1p(df["Entrance Fee in INR"])  # log(1 + x) to avoid log(0)
df["Number of google review in lakhs"] = np.log1p(df["Number of google review in lakhs"])

import seaborn as sns

# Box plots to check for outliers
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[["Entrance Fee in INR", "Number of google review in lakhs", "time needed to visit in hrs"]])
plt.xticks(rotation=45)
plt.title("Box Plot of Numerical Features")
plt.show()

# IQR Method to Remove Outliers from "Number of google review in lakhs"
Q1 = df["Number of google review in lakhs"].quantile(0.25)
Q3 = df["Number of google review in lakhs"].quantile(0.75)
IQR = Q3 - Q1

# Define lower and upper bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove outliers
df = df[(df["Number of google review in lakhs"] >= lower_bound) & (df["Number of google review in lakhs"] <= upper_bound)]


# IQR Method to Remove Outliers from "time needed to visit in hrs"
Q1 = df["time needed to visit in hrs"].quantile(0.25)
Q3 = df["time needed to visit in hrs"].quantile(0.75)
IQR = Q3 - Q1

# Define lower and upper bounds
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Remove outliers
df = df[(df["time needed to visit in hrs"] >= lower_bound) & (df["time needed to visit in hrs"] <= upper_bound)]

import seaborn as sns

# Box plots to check for outliers
plt.figure(figsize=(12, 6))
sns.boxplot(data=df[["Entrance Fee in INR", "Number of google review in lakhs", "time needed to visit in hrs"]])
plt.xticks(rotation=45)
plt.title("Box Plot of Numerical Features")
plt.show()

from sklearn.preprocessing import StandardScaler

# Selecting numerical features to scale
numerical_columns = ["time needed to visit in hrs", "Google review rating", "Entrance Fee in INR", "Number of google review in lakhs"]

# Standardizing the features
scaler = StandardScaler()
df[numerical_columns] = scaler.fit_transform(df[numerical_columns])

# Display the scaled dataset
df.head()

from sklearn.neighbors import NearestNeighbors

# Geographic popularity
df_popularity = df[["Zone", "Number of google review in lakhs"]]

# Compute nearest neighbors
neigh = NearestNeighbors(n_neighbors=5)
nbrs = neigh.fit(df_popularity)
distances, indices = nbrs.kneighbors(df_popularity)

# Sort distances for elbow method
sorted_distances = np.sort(distances[:, -1])

# Plot K-Distance Graph
plt.figure(figsize=(8, 5))
plt.plot(sorted_distances)
plt.xlabel("Points sorted by distance")
plt.ylabel("5th Nearest Neighbor Distance")
plt.title("Elbow Method for DBSCAN")
plt.show()

from sklearn.cluster import DBSCAN
import seaborn as sns


dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust eps based on elbow method

# Fit and predict clusters
df["Cluster_popularity"] = dbscan.fit_predict(df_popularity)

#Plotting
plt.figure(figsize=(15, 10))
sns.scatterplot(x=df["Number of google review in lakhs"], y=df["Zone"], hue=df["Cluster_popularity"], palette="viridis")
plt.xlabel("No. of Google Reviews")
plt.ylabel("Zone")
plt.title("Popularity by Region")
plt.show()

# Geographic Good / Bad Reviews
df_good_bad = df[["City", "Google review rating"]]

# Compute nearest neighbors
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(df_good_bad)
distances, indices = neighbors_fit.kneighbors(df_good_bad)

# Sort distances and plot
distances = np.sort(distances[:, 4])  # Taking the 5th nearest neighbor distance
plt.figure(figsize=(8, 5))
plt.plot(distances)
plt.xlabel("Data Points Sorted")
plt.ylabel("5th Nearest Neighbor Distance")
plt.title("K-Distance Graph for DBSCAN")
plt.show()

dbscan = DBSCAN(eps=3, min_samples=15)

# Fit and predict clusters
df["Cluster_good_bad"] = dbscan.fit_predict(df_good_bad)

#Plotting
plt.figure(figsize=(15, 10))
sns.scatterplot(x=df["City"], y=df["Google review rating"], hue=df["Cluster_good_bad"], palette="viridis")
plt.xlabel("City")
plt.ylabel("Ratings")
plt.title("Cities with Reviews")
plt.show()

# Geographic Good / Bad Reviews
df_significance = df[["Significance", "time needed to visit in hrs"]]

# Compute nearest neighbors
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(df_significance)
distances, indices = neighbors_fit.kneighbors(df_significance)

# Sort distances and plot
distances = np.sort(distances[:, 4])  # Taking the 5th nearest neighbor distance
plt.figure(figsize=(8, 5))
plt.plot(distances)
plt.xlabel("Data Points Sorted")
plt.ylabel("5th Nearest Neighbor Distance")
plt.title("K-Distance Graph for DBSCAN")
plt.show()

dbscan = DBSCAN(eps=0.8, min_samples=5)

# Fit and predict clusters
df["Cluster_significance"] = dbscan.fit_predict(df_good_bad)

#Plotting
plt.figure(figsize=(15, 10))
sns.scatterplot(x=df["time needed to visit in hrs"], y=df["Significance"], hue=df["Cluster_significance"], palette="viridis")
plt.xlabel("Visit Duration")
plt.ylabel("Significance")
plt.title("Time Required for Different Place Types")
plt.show()

# Geographic Good / Bad Reviews
df_cost = df[["Significance", "Entrance Fee in INR"]]

# Compute nearest neighbors
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(df_cost)
distances, indices = neighbors_fit.kneighbors(df_cost)

# Sort distances and plot
distances = np.sort(distances[:, 4])  # Taking the 5th nearest neighbor distance
plt.figure(figsize=(8, 5))
plt.plot(distances)
plt.xlabel("Data Points Sorted")
plt.ylabel("5th Nearest Neighbor Distance")
plt.title("K-Distance Graph for DBSCAN")
plt.show()

dbscan = DBSCAN(eps=0.7, min_samples=5)

# Fit and predict clusters
df["Cluster_cost"] = dbscan.fit_predict(df_cost)

#Plotting
plt.figure(figsize=(15, 10))
sns.scatterplot(x=df["Entrance Fee in INR"], y=df["Significance"], hue=df["Cluster_cost"], palette="viridis")
plt.xlabel("Entrance Fee")
plt.ylabel("Significance")
plt.title("Cost Comparison across Types")
plt.show()

# Review Based
df_expensive = df[["Google review rating", "Entrance Fee in INR"]]

# Compute nearest neighbors
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(df_expensive)
distances, indices = neighbors_fit.kneighbors(df_expensive)

# Sort distances and plot
distances = np.sort(distances[:, 4])  # Taking the 5th nearest neighbor distance
plt.figure(figsize=(8, 5))
plt.plot(distances)
plt.xlabel("Data Points Sorted")
plt.ylabel("5th Nearest Neighbor Distance")
plt.title("K-Distance Graph for DBSCAN")
plt.show()

dbscan = DBSCAN(eps=0.6, min_samples=3)

# Fit and predict clusters
df["Cluster_expensive"] = dbscan.fit_predict(df_expensive)

#Plotting
plt.figure(figsize=(15, 10))
sns.scatterplot(x=df["Entrance Fee in INR"], y=df["Google review rating"], hue=df["Cluster_expensive"], palette="viridis")
plt.xlabel("Entrance Fee")
plt.ylabel("Rating")
plt.title("Rating v/s Cost")
plt.show()

# Review Based
df_duration = df[["Number of google review in lakhs", "time needed to visit in hrs"]]

# Compute nearest neighbors
neighbors = NearestNeighbors(n_neighbors=5)
neighbors_fit = neighbors.fit(df_duration)
distances, indices = neighbors_fit.kneighbors(df_duration)

# Sort distances and plot
distances = np.sort(distances[:, 4])  # Taking the 5th nearest neighbor distance
plt.figure(figsize=(8, 5))
plt.plot(distances)
plt.xlabel("Data Points Sorted")
plt.ylabel("5th Nearest Neighbor Distance")
plt.title("K-Distance Graph for DBSCAN")
plt.show()

dbscan = DBSCAN(eps=0.6, min_samples=3)

# Fit and predict clusters
df["Cluster_duration"] = dbscan.fit_predict(df_duration)

#Plotting
plt.figure(figsize=(15, 10))
sns.scatterplot(x=df["Number of google review in lakhs"], y=df["time needed to visit in hrs"], hue=df["Cluster_duration"], palette="viridis")
plt.xlabel("Number of Google Reviews")
plt.ylabel("Visit Duration")
plt.title("Reviews v/s Duration")
plt.show()

df.head()

cluster_columns = ["Cluster_popularity", "Cluster_good_bad", "Cluster_significance",
                   "Cluster_cost", "Cluster_expensive", "Cluster_duration"]
titles = {
    "Cluster_popularity": "Popularity-Based Clustering",
    "Cluster_good_bad": "Good vs Bad Reviews Clustering",
    "Cluster_significance": "Significance-Based Clustering",
    "Cluster_cost": "Cost-Based Clustering",
    "Cluster_expensive": "Expensive Places Clustering",
    "Cluster_duration": "Visit Duration Clustering"
}

# Plots for Bar Charts
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 12))
axes = axes.flatten()

for i, col in enumerate(cluster_columns):
    sns.countplot(x=df[col], palette="viridis", ax=axes[i])
    axes[i].set_title(f"Cluster Distribution: {titles[col]}")
    axes[i].set_xlabel("Cluster Labels")
    axes[i].set_ylabel("Number of Places")

plt.tight_layout()
plt.show()

# Pie Charts for Proportion of Clusters
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))
axes = axes.flatten()

for i, col in enumerate(cluster_columns):
    df[col].value_counts().plot.pie(autopct="%1.1f%%", cmap="viridis", ax=axes[i], startangle=90)
    axes[i].set_ylabel("")
    axes[i].set_title(f"Cluster Proportion: {titles[col]}")

plt.tight_layout()
plt.show()

# Feature columns relevant to clusters for histograms
feature_columns = ["Number of google review in lakhs", "Google review rating", "Entrance Fee in INR", "time needed to visit in hrs"]

# Histograms to show feature distributions across clusters
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))
axes = axes.flatten()

for i, feature in enumerate(feature_columns):
    if feature in df.columns:
        sns.histplot(df, x=feature, hue=df["Cluster_popularity"], multiple="stack", palette="viridis", ax=axes[i])
        axes[i].set_title(f"Distribution of {feature} by Popularity Cluster")

plt.tight_layout()
plt.show()

